{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%%capture --no-stderr\n",
    "%pip install -U langgraph langsmith langchain_anthropic langchain_experimental tavily-python langchain_community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv(dotenv_path='prod.env')\n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = os.getenv('OPENAI_API_KEY')\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = os.getenv('LANGCHAIN_API_KEY')\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.langchain.plus\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = \"Code Analysis AI Tool\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 01:53:49.668 WARNING streamlit.runtime.scriptrunner_utils.script_run_context: Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from typing import Annotated\n",
    "\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "from langchain_core.tools import tool\n",
    "from langchain_experimental.utilities import PythonREPL\n",
    "\n",
    "from typing import Annotated\n",
    "\n",
    "from langchain_anthropic import ChatAnthropic\n",
    "from langchain_openai import ChatOpenAI\n",
    "from typing_extensions import TypedDict\n",
    "\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "\n",
    "import json\n",
    "\n",
    "from langchain_core.messages import ToolMessage\n",
    "\n",
    "from typing import Literal\n",
    "\n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import streamlit as st\n",
    "from streamlit_ace import st_ace, LANGUAGES, THEMES, KEYBINDINGS\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_openai import OpenAI\n",
    "from langchain_core.prompts.chat import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain.output_parsers import RegexParser\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 01:54:56.771 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:56.773 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:56.776 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:56.778 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:56.782 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:56.784 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:56.785 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    }
   ],
   "source": [
    "st.set_page_config(page_title=\"Code Generation and Analysis AI Tool\", layout=\"wide\")\n",
    "st.title(\"Code Generation and Analysis AI Tool\")\n",
    "st.write(\"Write your code in the editor below. Click **Generate Suggestion** to see the AI's suggestion.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 01:54:59.495 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:59.496 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:59.498 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:59.501 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:59.502 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n",
      "2024-12-08 01:54:59.503 Thread 'MainThread': missing ScriptRunContext! This warning can be ignored when running in bare mode.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Successfully executed:\\n```python\\nprint('Hello World')\\n```\\nStdout: Hello World\\n\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\""
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_analysis_prompt_template = \"\"\"You are a coding assistant who helps users to analyze and improve their code. Use the following pieces of context to answer the question at the end. If you don't know the answer, just say that you don't know, don't try to make up an answer.\n",
    "When you answer with the suggested changes, make sure you also include the relevant code snippets in the relevant language and provide each suggestion and code snippet on a separate line.\n",
    "The output should be in the following format:\n",
    "\n",
    "Question: [question here]\n",
    "Helpful Answer: [answer here]\n",
    "Score: [score between 0 and 100]\n",
    "\n",
    "Begin!\n",
    "\n",
    "Context:\n",
    "---------\n",
    "{context}\n",
    "---------\n",
    "Question: {question}\n",
    "Helpful Answer:\"\"\"\n",
    "\n",
    "output_parser = RegexParser(\n",
    "    regex=r\"(.*?)\\nScore: (.*)\",\n",
    "    output_keys=[\"answer\", \"score\"],\n",
    ")\n",
    "\n",
    "PROMPT = PromptTemplate(\n",
    "    template=code_analysis_prompt_template,\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    output_parser=output_parser\n",
    ")\n",
    "\n",
    "code_analysis_chain = load_qa_chain(OpenAI(temperature=0), chain_type=\"map_rerank\", return_intermediate_steps=True, prompt=PROMPT)\n",
    "\n",
    "@st.cache_resource\n",
    "def load_and_split_code(folder_path):\n",
    "    loader = DirectoryLoader(folder_path, glob=\"**/*.*\")\n",
    "    documents = loader.load()\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1200, chunk_overlap=100, length_function=len)\n",
    "    return text_splitter.split_documents(documents)\n",
    "\n",
    "@st.cache_resource\n",
    "def create_embeddings(_texts):\n",
    "    embeddings = OpenAIEmbeddings()\n",
    "    vector_store = FAISS.from_documents(_texts, embeddings)\n",
    "    return vector_store\n",
    "\n",
    "code_folder = st.text_input(\"Enter the path to the folder containing your code files (optional):\")\n",
    "enable_analysis = False\n",
    "if code_folder:\n",
    "    if os.path.exists(code_folder) and os.path.isdir(code_folder):\n",
    "        st.success(f\"Folder `{code_folder}` selected.\")\n",
    "        enable_analysis = True\n",
    "    else:\n",
    "        st.error(\"Invalid folder path. Please check and try again.\")\n",
    "\n",
    "if enable_analysis:\n",
    "    with st.spinner(\"Loading and splitting code files...\"):\n",
    "        print(code_folder)\n",
    "        texts = load_and_split_code(code_folder)\n",
    "\n",
    "    with st.spinner(\"Creating embeddings...\"):\n",
    "        vector_store = create_embeddings(texts)\n",
    "\n",
    "if enable_analysis:\n",
    "    st.subheader(\"Query Your Codebase\")\n",
    "    query = st.text_input(\"Enter your question or search term:\")\n",
    "    if query:\n",
    "        with st.spinner(\"Searching for relevant code...\"):\n",
    "            relevant_chunks = vector_store.similarity_search_with_score(query, k=2)\n",
    "            chunk_docs = [chunk[0] for chunk in relevant_chunks]\n",
    "            results = code_analysis_chain({\"input_documents\": chunk_docs, \"question\": query})\n",
    "            text_reference = \"\".join([doc.page_content for doc in results[\"input_documents\"]])\n",
    "            print(f\"Answer: {results['output_text']}\\n\\nReference: {text_reference}\")\n",
    "            st.markdown(f\"```Answer: {results[\"output_text\"]}```\")\n",
    "\n",
    "\n",
    "@tool\n",
    "def code_analysis_tool(\n",
    "    code: Annotated[str, \"The python code to execute to generate your chart.\"],\n",
    "):\n",
    "    \"\"\"Use this to execute python code. If you want to see the output of a value,\n",
    "    you should print it out with `print(...)`. This is visible to the user.\"\"\"\n",
    "    try:\n",
    "        result = repl.run(code)\n",
    "    except BaseException as e:\n",
    "        return f\"Failed to execute. Error: {repr(e)}\"\n",
    "    result_str = f\"Successfully executed:\\n```python\\n{code}\\n```\\nStdout: {result}\"\n",
    "    return (\n",
    "        result_str + \"\\n\\nIf you have completed all tasks, respond with FINAL ANSWER.\"\n",
    "    )\n",
    "\n",
    "code_analysis_tool.invoke(\"print('Hello World')\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [code_analysis_tool]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x289ffebda00>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class State(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "\n",
    "graph_builder = StateGraph(State)\n",
    "\n",
    "llm = ChatOpenAI(model_name = 'gpt-4o', temperature = 0.3)\n",
    "# Modification: tell the LLM which tools it can call\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "\n",
    "def chatbot(state: State):\n",
    "    return {\"messages\": [llm_with_tools.invoke(state[\"messages\"])]}\n",
    "\n",
    "\n",
    "graph_builder.add_node(\"chatbot\", chatbot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langgraph.graph.state.StateGraph at 0x289ffebda00>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BasicToolNode:\n",
    "    \"\"\"A node that runs the tools requested in the last AIMessage.\"\"\"\n",
    "\n",
    "    def __init__(self, tools: list) -> None:\n",
    "        self.tools_by_name = {tool.name: tool for tool in tools}\n",
    "\n",
    "    def __call__(self, inputs: dict):\n",
    "        if messages := inputs.get(\"messages\", []):\n",
    "            message = messages[-1]\n",
    "        else:\n",
    "            raise ValueError(\"No message found in input\")\n",
    "        outputs = []\n",
    "        for tool_call in message.tool_calls:\n",
    "            tool_result = self.tools_by_name[tool_call[\"name\"]].invoke(\n",
    "                tool_call[\"args\"]\n",
    "            )\n",
    "            outputs.append(\n",
    "                ToolMessage(\n",
    "                    content=json.dumps(tool_result),\n",
    "                    name=tool_call[\"name\"],\n",
    "                    tool_call_id=tool_call[\"id\"],\n",
    "                )\n",
    "            )\n",
    "        return {\"messages\": outputs}\n",
    "\n",
    "\n",
    "tool_node = BasicToolNode(tools=tools)\n",
    "graph_builder.add_node(\"tools\", tool_node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def route_tools(\n",
    "    state: State,\n",
    "):\n",
    "    \"\"\"\n",
    "    Use in the conditional_edge to route to the ToolNode if the last message\n",
    "    has tool calls. Otherwise, route to the end.\n",
    "    \"\"\"\n",
    "    if isinstance(state, list):\n",
    "        ai_message = state[-1]\n",
    "    elif messages := state.get(\"messages\", []):\n",
    "        ai_message = messages[-1]\n",
    "    else:\n",
    "        raise ValueError(f\"No messages found in input state to tool_edge: {state}\")\n",
    "    if hasattr(ai_message, \"tool_calls\") and len(ai_message.tool_calls) > 0:\n",
    "        return \"tools\"\n",
    "    return END\n",
    "\n",
    "\n",
    "# The `tools_condition` function returns \"tools\" if the chatbot asks to use a tool, and \"END\" if\n",
    "# it is fine directly responding. This conditional routing defines the main agent loop.\n",
    "graph_builder.add_conditional_edges(\n",
    "    \"chatbot\",\n",
    "    route_tools,\n",
    "    # The following dictionary lets you tell the graph to interpret the condition's outputs as a specific node\n",
    "    # It defaults to the identity function, but if you\n",
    "    # want to use a node named something else apart from \"tools\",\n",
    "    # You can update the value of the dictionary to something else\n",
    "    # e.g., \"tools\": \"my_tools\"\n",
    "    {\"tools\": \"tools\", END: END},\n",
    ")\n",
    "# Any time a tool is called, we return to the chatbot to decide the next step\n",
    "graph_builder.add_edge(\"tools\", \"chatbot\")\n",
    "graph_builder.add_edge(START, \"chatbot\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: The Eiffel Tower is approximately 324 meters (1,063 feet) tall, including its antennas.\n",
      "Assistant: Certainly! Here's a Python implementation of the merge sort algorithm:\n",
      "\n",
      "```python\n",
      "def merge_sort(arr):\n",
      "    if len(arr) > 1:\n",
      "        mid = len(arr) // 2\n",
      "        left_half = arr[:mid]\n",
      "        right_half = arr[mid:]\n",
      "\n",
      "        # Recursive call on each half\n",
      "        merge_sort(left_half)\n",
      "        merge_sort(right_half)\n",
      "\n",
      "        # Two iterators for traversing the two halves\n",
      "        i = 0\n",
      "        j = 0\n",
      "        \n",
      "        # Iterator for the main list\n",
      "        k = 0\n",
      "        \n",
      "        # Until we reach either end of either left_half or right_half\n",
      "        while i < len(left_half) and j < len(right_half):\n",
      "            if left_half[i] < right_half[j]:\n",
      "                # The value from the left_half has been used\n",
      "                arr[k] = left_half[i]\n",
      "                # Move the iterator forward\n",
      "                i += 1\n",
      "            else:\n",
      "                arr[k] = right_half[j]\n",
      "                j += 1\n",
      "            # Move to the next slot in the main list\n",
      "            k += 1\n",
      "\n",
      "        # For all the remaining values\n",
      "        while i < len(left_half):\n",
      "            arr[k] = left_half[i]\n",
      "            i += 1\n",
      "            k += 1\n",
      "\n",
      "        while j < len(right_half):\n",
      "            arr[k] = right_half[j]\n",
      "            j += 1\n",
      "            k += 1\n",
      "\n",
      "# Example usage\n",
      "arr = [12, 11, 13, 5, 6, 7]\n",
      "merge_sort(arr)\n",
      "print(\"Sorted array is:\", arr)\n",
      "```\n",
      "\n",
      "You can use this code to sort an array by calling the `merge_sort` function with the array you want to sort. The example usage shows how to sort a sample array.\n",
      "Goodbye!\n"
     ]
    }
   ],
   "source": [
    "def stream_graph_updates(user_input: str):\n",
    "    for event in graph.stream({\"messages\": [(\"user\", user_input)]}):\n",
    "        for value in event.values():\n",
    "            print(\"Assistant:\", value[\"messages\"][-1].content)\n",
    "            \n",
    "while True:\n",
    "    try:\n",
    "        user_input = input(\"User: \")\n",
    "        if user_input.lower() in [\"quit\", \"exit\", \"q\"]:\n",
    "            print(\"Goodbye!\")\n",
    "            break\n",
    "\n",
    "        stream_graph_updates(user_input)\n",
    "    except:\n",
    "        # fallback if input() is not available\n",
    "        user_input = \"What do you know about LangGraph?\"\n",
    "        print(\"User: \" + user_input)\n",
    "        stream_graph_updates(user_input)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
